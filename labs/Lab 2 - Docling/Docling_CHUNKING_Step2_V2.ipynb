{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f70cd3e-2c3b-4b77-8d0b-f0d23aee19bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurvindersingh/Documents/development/learning/watsonx-workshop/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from warnings import filterwarnings\n",
    "from rich.pretty import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import base64\n",
    "from pydantic import BaseModel\n",
    "# from io import BytesIO\n",
    "# import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "import requests\n",
    "\n",
    "import PyPDF2\n",
    "from pdf2image import convert_from_bytes\n",
    "from IPython.display import display, Markdown, HTML, display_html\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# from llama_index.core import StorageContext, VectorStoreIndex\n",
    "# from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from llama_index.core import Document as LIDocument\n",
    "\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "from docling.datamodel.document import ConversionResult\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.models.ocr_mac_model import OcrMacOptions\n",
    "from docling.models.tesseract_ocr_cli_model import TesseractCliOcrOptions\n",
    "#from docling.models.tesseract_ocr_model import TesseractOcrOptions\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    EasyOcrOptions,\n",
    "    OcrMacOptions,\n",
    "    PdfPipelineOptions,\n",
    "    RapidOcrOptions,\n",
    "    TesseractCliOcrOptions,\n",
    "    TesseractOcrOptions,\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    ")\n",
    "\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "#from docling.backend.docling_parse_backend import DoclingParseDocumentBackend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832891d1-aa11-4013-8a23-99847822d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "def _get_env_from_colab_or_os(key):\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        try:\n",
    "            return userdata.get(key)\n",
    "        except userdata.SecretNotFoundError:\n",
    "            pass\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return os.getenv(key)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
    "# https://github.com/huggingface/transformers/issues/5486:\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"../results/extract\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "#### PLEASE PROVIDE THE PATH OF SOURCE DOCUMENT\n",
    "# SOURCE = \"/Users/gurvindersingh/Library/CloudStorage/Box-Box/MyDocs/2024/GSIs/NTTData/BoI/docs/input_output/FinancialReport1.pdf\"\n",
    "# SOURCE = \"/Users/gurvindersingh/Documents/development/datasets/FAQs/US_Visa_FAQ.pdf\"\n",
    "SOURCE = \"./pdfs/US_Visa_FAQ.pdf\"\n",
    "pdf_file_path = SOURCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f89377-8db2-403f-b8d0-e92189860c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from docling_core.types.doc.document import DoclingDocument as DLDocument\n",
    "\n",
    "def read_parquet(file_path):\n",
    "    document_content_df = pd.read_parquet(file_path)\n",
    "    tempStr: str = document_content_df.to_json(orient='records')\n",
    "    tempObj = json.loads(tempStr)\n",
    "    jsonString = json.dumps(tempObj[0][\"0\"])\n",
    "    jsonObj = json.loads(jsonString)\n",
    "    # print(jsonObj)\n",
    "    # pprint(jsonObj, max_length=5, max_string=50, max_depth=4)       \n",
    "    dl_output: DLDocument = DLDocument.model_validate_json(jsonObj)    \n",
    "    return dl_output\n",
    "\n",
    "# file_name = \"PRINTER_cpd58007\"\n",
    "collection_name = os.path.basename(SOURCE).split('/')[-1].split('.')[0]\n",
    "parquet_file_path = OUTPUT_DIR / f\"{collection_name}.parquet\"\n",
    "dl_obj = read_parquet(parquet_file_path)\n",
    "# print(f\"\\n\\n{dl_obj.export_to_markdown()}\\n\\n\")\n",
    "# pprint(dl_obj, max_length=10, max_string=50, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a4f6a-40a5-4e07-84cf-be403aaa3942",
   "metadata": {},
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b46cf1f-8035-4d7d-a9ec-bc30eb6cc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "from docling_core.types.doc.document import (\n",
    "    DocItem,\n",
    "    DocumentOrigin,\n",
    "    LevelNumber,\n",
    "    ListItem,\n",
    "    SectionHeaderItem,\n",
    "    TableItem,\n",
    "    TextItem,\n",
    "    PictureItem\n",
    ")\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "from docling_core.types.doc.base import (\n",
    "    CoordOrigin,\n",
    "    Size,\n",
    "    BoundingBox\n",
    ")\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "def convert_to_image(pdf_path, page_num):\n",
    "    pdf_file = open(pdf_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    page = pdf_reader.pages[page_num]\n",
    "   \n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "    pdf_writer.add_page(page)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    pdf_writer.write(buffer)\n",
    "    \n",
    "    # Convert output PDF data to image and save as PNG file\n",
    "    images = convert_from_bytes(buffer.getvalue())\n",
    "    buffer.close()    \n",
    "    pdf_file.close()\n",
    "    \n",
    "    return images[0]\n",
    "\n",
    "def crop_image_from_pdf(page_image, page_size: Size, crop_coords):\n",
    "    \"\"\"Crops an image from a PDF page.\"\"\"\n",
    "    # page_image = convert_to_image(pdf_path, page_num)\n",
    "    page_image = page_image.resize((int(page_size.width), int(page_size.height)))\n",
    "    cropped_im = page_image.crop(crop_coords.as_tuple())\n",
    "    return cropped_im\n",
    "\n",
    "def convert_to_pil_image(plt):\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    pil_img = copy.deepcopy(Image.open(buf))   \n",
    "    buf.close()\n",
    "    return pil_img\n",
    "\n",
    "def convert_to_img_bytes(pil_img):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    return img_byte_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7758e0-50e5-4f11-9b44-8ba70fa4ec78",
   "metadata": {},
   "source": [
    "### Document Chunking (Llamaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8460cc21-f624-4a8d-a7e8-a245de147d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'c8b76dfb-76f7-458a-9f23-e729a6fac238'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">text_resource</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MediaResource</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">embeddings</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"schema_name\": \"DoclingDocument\", \"version\": \"1.2'</span>+<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">522781</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">image_resource</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">audio_resource</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">video_resource</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mid_\u001b[0m=\u001b[32m'c8b76dfb-76f7-458a-9f23-e729a6fac238'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mtext_resource\u001b[0m=\u001b[1;35mMediaResource\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33membeddings\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdata\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtext\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"schema_name\": \"DoclingDocument\", \"version\": \"1.2'\u001b[0m+\u001b[1;36m522781\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mpath\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33murl\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mmimetype\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mimage_resource\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33maudio_resource\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mvideo_resource\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "from llama_index.core import Document as LIDocument\n",
    "\n",
    "file_name = os.path.basename(SOURCE).split('/')[-1].split('.')[0]\n",
    "parquet_file_path = OUTPUT_DIR / f\"{file_name}.parquet\"\n",
    "dl_obj = read_parquet(parquet_file_path)\n",
    "\n",
    "copied_dl_obj2 = copy.deepcopy(dl_obj)\n",
    "extra_info = None\n",
    "\n",
    "def _uuid4_doc_id_gen(doc: DLDocument, file_path: str | Path) -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# SOURCE = \"../datasets/pdfs/other/DocLaynetPaper.pdf\"\n",
    "\n",
    "text = json.dumps(copied_dl_obj2.export_to_dict())\n",
    "\n",
    "li_doc = LIDocument(\n",
    "                doc_id=_uuid4_doc_id_gen(doc=copied_dl_obj2, file_path=SOURCE),\n",
    "                text=text,\n",
    "            )\n",
    "li_doc.metadata = extra_info or {}\n",
    "\n",
    "documents = [li_doc]\n",
    "\n",
    "\n",
    "## OR DIRECTLY USING DOCLING READER\n",
    "\n",
    "\n",
    "# pipeline_options = PdfPipelineOptions()\n",
    "# pipeline_options.do_ocr = True\n",
    "# pipeline_options.do_table_structure = True\n",
    "# pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "# doc_converter = DocumentConverter(\n",
    "#     format_options={\n",
    "#         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # doc_converter: DocumentConverter = Field(default_factory=DocumentConverter)\n",
    "# reader = DoclingReader(export_type=DoclingReader.ExportType.JSON, doc_converter=doc_converter)\n",
    "# documents=reader.load_data(SOURCE)\n",
    "\n",
    "\n",
    "# reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)\n",
    "# documents=reader.load_data(SOURCE)\n",
    "\n",
    "pprint(documents, max_length=10, max_string=50, max_depth=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9606c296-1077-4ae1-8e31-8603015f7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright IBM Corp. 2024 - 2025\n",
    "# SPDX-License-Identifier: MIT\n",
    "#\n",
    "\n",
    "\"\"\"Chunker implementation leveraging the document structure.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from typing import Any, ClassVar, Final, Iterator, Literal, Optional\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pydantic import Field, StringConstraints, field_validator\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from docling_core.search.package import VERSION_PATTERN\n",
    "from docling_core.transforms.chunker import BaseChunk, BaseChunker, BaseMeta\n",
    "from docling_core.types import DoclingDocument as DLDocument\n",
    "from docling_core.types.doc.document import (\n",
    "    DocItem,\n",
    "    DocumentOrigin,\n",
    "    LevelNumber,\n",
    "    ListItem,\n",
    "    SectionHeaderItem,\n",
    "    TableItem,\n",
    "    PictureItem,\n",
    "    TextItem,\n",
    ")\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "\n",
    "_VERSION: Final = \"1.0.0\"\n",
    "\n",
    "_KEY_SCHEMA_NAME = \"schema_name\"\n",
    "_KEY_VERSION = \"version\"\n",
    "_KEY_DOC_ITEMS = \"doc_items\"\n",
    "_KEY_HEADINGS = \"headings\"\n",
    "_KEY_CAPTIONS = \"captions\"\n",
    "_KEY_ORIGIN = \"origin\"\n",
    "_KEY_NODE_TYPE = \"node_type\"\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DocMeta(BaseMeta):\n",
    "    \"\"\"Data model for Hierarchical Chunker chunk metadata.\"\"\"\n",
    "\n",
    "    schema_name: Literal[\"docling_core.transforms.chunker.DocMeta\"] = Field(\n",
    "        default=\"docling_core.transforms.chunker.DocMeta\",\n",
    "        alias=_KEY_SCHEMA_NAME,\n",
    "    )\n",
    "    version: Annotated[str, StringConstraints(pattern=VERSION_PATTERN, strict=True)] = (\n",
    "        Field(\n",
    "            default=_VERSION,\n",
    "            alias=_KEY_VERSION,\n",
    "        )\n",
    "    )\n",
    "    doc_items: list[DocItem] = Field(\n",
    "        alias=_KEY_DOC_ITEMS,\n",
    "        min_length=1,\n",
    "    )\n",
    "    headings: Optional[list[str]] = Field(\n",
    "        default=None,\n",
    "        alias=_KEY_HEADINGS,\n",
    "        min_length=1,\n",
    "    )\n",
    "    captions: Optional[list[str]] = Field(\n",
    "        default=None,\n",
    "        alias=_KEY_CAPTIONS,\n",
    "        min_length=1,\n",
    "    )\n",
    "    origin: Optional[DocumentOrigin] = Field(\n",
    "        default=None,\n",
    "        alias=_KEY_ORIGIN,\n",
    "    )\n",
    "\n",
    "    excluded_embed: ClassVar[list[str]] = [\n",
    "        _KEY_SCHEMA_NAME,\n",
    "        _KEY_VERSION,\n",
    "        _KEY_DOC_ITEMS,\n",
    "        _KEY_ORIGIN,\n",
    "    ]\n",
    "    excluded_llm: ClassVar[list[str]] = [\n",
    "        _KEY_SCHEMA_NAME,\n",
    "        _KEY_VERSION,\n",
    "        _KEY_DOC_ITEMS,\n",
    "        _KEY_ORIGIN,\n",
    "    ]\n",
    "\n",
    "    node_type: Optional[str] = Field(\n",
    "        default=None,\n",
    "        alias=_KEY_NODE_TYPE,\n",
    "    )\n",
    "\n",
    "    @field_validator(_KEY_VERSION)\n",
    "    @classmethod\n",
    "    def check_version_is_compatible(cls, v: str) -> str:\n",
    "        \"\"\"Check if this meta item version is compatible with current version.\"\"\"\n",
    "        current_match = re.match(VERSION_PATTERN, _VERSION)\n",
    "        doc_match = re.match(VERSION_PATTERN, v)\n",
    "        if (\n",
    "            doc_match is None\n",
    "            or current_match is None\n",
    "            or doc_match[\"major\"] != current_match[\"major\"]\n",
    "            or doc_match[\"minor\"] > current_match[\"minor\"]\n",
    "        ):\n",
    "            raise ValueError(f\"incompatible version {v} with schema version {_VERSION}\")\n",
    "        else:\n",
    "            return _VERSION\n",
    "\n",
    "\n",
    "class DocChunk(BaseChunk):\n",
    "    \"\"\"Data model for Hierarchical Chunker chunks.\"\"\"\n",
    "\n",
    "    meta: DocMeta\n",
    "\n",
    "\n",
    "class CustomHierarchicalChunker(BaseChunker):\n",
    "    r\"\"\"Chunker implementation leveraging the document layout.\n",
    "\n",
    "    Args:\n",
    "        merge_list_items (bool): Whether to merge successive list items.\n",
    "            Defaults to True.\n",
    "        delim (str): Delimiter to use for merging text. Defaults to \"\\n\".\n",
    "    \"\"\"\n",
    "\n",
    "    merge_list_items: bool = True\n",
    "    delim: str = \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def _clean_text(cls, text):\n",
    "        cleaned_text = text.strip()\n",
    "        cleaned_text = cleaned_text.replace(\". . .\", \"\")\n",
    "        cleaned_text = cleaned_text.replace(\"--\", \"\")\n",
    "        cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "        cleaned_text = cleaned_text.replace(\"(\", \" \")\n",
    "        cleaned_text = cleaned_text.replace(\")\", \" \")\n",
    "        cleaned_text = cleaned_text.replace(\"\\xa0\", \" \")\n",
    "        cleaned_text = cleaned_text.replace(\"\\uf0b7\", \"l\")\n",
    "        cleaned_text = cleaned_text.replace(\"/s/\", \"\")\n",
    "        return cleaned_text\n",
    "\n",
    "    @classmethod\n",
    "    def _triplet_serialize(cls, table_df: DataFrame) -> str:\n",
    "\n",
    "        # copy header as first row and shift all rows by one\n",
    "        table_df.loc[-1] = table_df.columns  # type: ignore[call-overload]\n",
    "        table_df.index = table_df.index + 1\n",
    "        table_df = table_df.sort_index()\n",
    "\n",
    "        rows = [str(item).strip() for item in table_df.iloc[:, 0].to_list()]\n",
    "        cols = [str(item).strip() for item in table_df.iloc[0, :].to_list()]\n",
    "\n",
    "        nrows = table_df.shape[0]\n",
    "        ncols = table_df.shape[1]\n",
    "        texts = [\n",
    "            f\"{rows[i]}, {cols[j]} = {str(table_df.iloc[i, j]).strip()}\"\n",
    "            for i in range(1, nrows)\n",
    "            for j in range(1, ncols)\n",
    "        ]\n",
    "        output_text = \". \".join(texts)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def chunk(self, dl_doc: DLDocument, **kwargs: Any) -> Iterator[BaseChunk]:\n",
    "        r\"\"\"Chunk the provided document.\n",
    "\n",
    "        Args:\n",
    "            dl_doc (DLDocument): document to chunk\n",
    "\n",
    "        Yields:\n",
    "            Iterator[Chunk]: iterator over extracted chunks\n",
    "        \"\"\"\n",
    "        heading_by_level: dict[LevelNumber, str] = {}\n",
    "        list_items: list[TextItem] = []\n",
    "        for item, level in dl_doc.iterate_items():\n",
    "            captions = None\n",
    "            node_type = None\n",
    "            if isinstance(item, DocItem):\n",
    "                node_type = type(item).__name__\n",
    "                # first handle any merging needed\n",
    "                if self.merge_list_items:\n",
    "                    if isinstance(\n",
    "                        item, ListItem\n",
    "                    ) or (  # TODO remove when all captured as ListItem:\n",
    "                        isinstance(item, TextItem)\n",
    "                        and item.label == DocItemLabel.LIST_ITEM\n",
    "                    ):\n",
    "                        list_items.append(item)\n",
    "                        continue\n",
    "                    elif list_items:  # need to yield\n",
    "                        yield DocChunk(\n",
    "                            text=self.delim.join([i.text for i in list_items]),\n",
    "                            meta=DocMeta(\n",
    "                                doc_items=list_items,\n",
    "                                headings=[\n",
    "                                    heading_by_level[k]\n",
    "                                    for k in sorted(heading_by_level)\n",
    "                                ]\n",
    "                                or None,\n",
    "                                origin=dl_doc.origin,\n",
    "                                node_type=node_type\n",
    "                            ),\n",
    "                        )\n",
    "                        list_items = []  # reset\n",
    "\n",
    "                if isinstance(item, SectionHeaderItem) or (\n",
    "                    isinstance(item, TextItem)\n",
    "                    and item.label in [DocItemLabel.SECTION_HEADER, DocItemLabel.TITLE]\n",
    "                ):\n",
    "                    level = (\n",
    "                        item.level\n",
    "                        if isinstance(item, SectionHeaderItem)\n",
    "                        else (0 if item.label == DocItemLabel.TITLE else 1)\n",
    "                    )\n",
    "                    heading_by_level[level] = item.text\n",
    "\n",
    "                    # remove headings of higher level as they just went out of scope\n",
    "                    keys_to_del = [k for k in heading_by_level if k > level]\n",
    "                    for k in keys_to_del:\n",
    "                        heading_by_level.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "                if isinstance(item, TextItem) or (\n",
    "                    (not self.merge_list_items) and isinstance(item, ListItem)\n",
    "                ):\n",
    "                    text = item.text\n",
    "                elif isinstance(item, TableItem):\n",
    "                    table_df = item.export_to_dataframe()\n",
    "                    if table_df.shape[0] < 1 or table_df.shape[1] < 2:\n",
    "                        # at least two cols needed, as first column contains row headers\n",
    "                        continue\n",
    "                    # text = self._triplet_serialize(table_df=table_df)\n",
    "                    text = table_df.to_markdown()\n",
    "                    # text = self._clean_text(text)\n",
    "                    captions = [\n",
    "                        c.text for c in [r.resolve(dl_doc) for r in item.captions]\n",
    "                    ] or None\n",
    "                elif isinstance(item, PictureItem):\n",
    "                    if item.annotations is not None and len(item.annotations) > 0:\n",
    "                        text = \", \".join([annotation.text for annotation in item.annotations])\n",
    "                        text = \"PICTURE METADATA: \" + text\n",
    "                    else:\n",
    "                        text = \"NO PICTURE METADATA\"\n",
    "                        \n",
    "                    captions = [\n",
    "                        c.text for c in [r.resolve(dl_doc) for r in item.captions]\n",
    "                    ] or None\n",
    "                else:\n",
    "                    continue\n",
    "                   \n",
    "                c = DocChunk(\n",
    "                    text=text,\n",
    "                    meta=DocMeta(\n",
    "                        doc_items=[item],\n",
    "                        headings=[heading_by_level[k] for k in sorted(heading_by_level)]\n",
    "                        or None,\n",
    "                        captions=captions,\n",
    "                        origin=dl_doc.origin,\n",
    "                        node_type=node_type\n",
    "                    ),\n",
    "                )\n",
    "                yield c\n",
    "\n",
    "        if self.merge_list_items and list_items:  # need to yield\n",
    "            yield DocChunk(\n",
    "                text=self.delim.join([i.text for i in list_items]),\n",
    "                meta=DocMeta(\n",
    "                    doc_items=list_items,\n",
    "                    headings=[heading_by_level[k] for k in sorted(heading_by_level)]\n",
    "                    or None,\n",
    "                    origin=dl_doc.origin,\n",
    "                ),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea263ef-fdb8-4d6c-b14a-61bfadb6f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable, Protocol, Sequence, runtime_checkable\n",
    "import uuid\n",
    "import copy\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from llama_index.core.schema import Document as LIDocument\n",
    "from llama_index.core.node_parser import NodeParser\n",
    "\n",
    "from docling_core.transforms.chunker import BaseChunker, HierarchicalChunker\n",
    "from docling_core.types import DoclingDocument as DLDocument\n",
    "from llama_index.core import Document as LIDocument\n",
    "from llama_index.core.node_parser import NodeParser\n",
    "from llama_index.core.schema import (\n",
    "    BaseNode,\n",
    "    NodeRelationship,\n",
    "    RelatedNodeType,\n",
    "    TextNode,\n",
    ")\n",
    "from llama_index.core.utils import get_tqdm_iterable\n",
    "\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "class ChunkConfig(BaseModel):   \n",
    "    split_at: str\n",
    "    chunk_size: int\n",
    "    chunk_overlap: int \n",
    "    \n",
    "    def __init__(self, chunk_size: int, chunk_overlap: int, split_at: str = \"HEADER\"):\n",
    "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, split_at=split_at)\n",
    "        self.split_at = split_at\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap  \n",
    "\n",
    "class CustomNodeParser(NodeParser):\n",
    "    \"\"\"Docling format node parser.\n",
    "\n",
    "    Splits the JSON format of `DoclingReader` into nodes corresponding\n",
    "    to respective document elements from Docling's data model\n",
    "    (paragraphs, headings, tables etc.).\n",
    "\n",
    "    Args:\n",
    "        chunker (BaseChunker, optional): The chunker to use. Defaults to `HierarchicalChunker()`.\n",
    "        id_func(NodeIDGenCallable, optional): The node ID generation function to use. Defaults to `_uuid4_node_id_gen`.\n",
    "    \"\"\"\n",
    "\n",
    "    @runtime_checkable\n",
    "    class NodeIDGenCallable(Protocol):\n",
    "        def __call__(self, i: int, node: BaseNode) -> str:\n",
    "            ...\n",
    "\n",
    "    @staticmethod\n",
    "    def _uuid4_node_id_gen(i: int, node: BaseNode) -> str:\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    chunker: BaseChunker = HierarchicalChunker()\n",
    "    id_func: NodeIDGenCallable = _uuid4_node_id_gen\n",
    "    chunk_config: ChunkConfig = ChunkConfig(chunk_size=512, chunk_overlap=30)\n",
    "\n",
    "    def _split_by_header(self, chunks):\n",
    "        updated_chunks = []\n",
    "        for ix, chunk in enumerate(chunks):\n",
    "            if len(chunk.text) < 7:\n",
    "                ## We dont need very small text nodes (like page numbers etc.)\n",
    "                # print(f\"REMOVING CHUNK AS THE TEXT SIZE IS VERY SMALL: {chunk.text}\")\n",
    "                continue \n",
    "\n",
    "            header = None\n",
    "            if 'headings' in chunk.meta or chunk.meta.headings:\n",
    "                header = \" >> \".join(chunk.meta.headings)\n",
    "\n",
    "            # if chunk.meta.node_type == \"TableItem\":\n",
    "            #     updated_chunks.append(chunk)\n",
    "            #     continue\n",
    "\n",
    "            if len(updated_chunks) > 0:\n",
    "                previousChunk = updated_chunks.pop()\n",
    "                # if previousChunk.meta.node_type == \"TableItem\":\n",
    "                #     updated_chunks.append(previousChunk)\n",
    "                #     updated_chunks.append(chunk)\n",
    "                #     continue\n",
    "                \n",
    "                if previousChunk.meta.headings == chunk.meta.headings and previousChunk.text and chunk.text:\n",
    "                    if 'NO PICTURE METADATA' in previousChunk.text:\n",
    "                        # previousChunk.text = f\"{chunk.text}\"\n",
    "                        previousChunk.text = previousChunk.text.replace('NO PICTURE METADATA', '<--  IMAGE -->')                       \n",
    "                    \n",
    "                    previousChunk.text = f\"{previousChunk.text}\\n\\n{chunk.text}\"\n",
    "                    \n",
    "                    if previousChunk.meta.doc_items and chunk.meta.doc_items:\n",
    "                        previousChunk.meta.doc_items.extend(chunk.meta.doc_items)\n",
    "                    if previousChunk.meta.captions and chunk.meta.captions:\n",
    "                        previousChunk.meta.captions.extend(chunk.meta.captions)\n",
    "                    if previousChunk.meta.node_type == \"TableItem\" or chunk.meta.node_type == \"TableItem\":\n",
    "                        previousChunk.meta.node_type = \"TableItem\"\n",
    "                        \n",
    "                    updated_chunks.append(previousChunk)\n",
    "                    # print(f\"## CHUNKS MERGED, new text length: {len(previousChunk.text)}, Headings: {previousChunk.meta.headings}\\n\")\n",
    "                else:\n",
    "                    updated_chunks.append(previousChunk)\n",
    "                    updated_chunks.append(chunk)\n",
    "            else:\n",
    "                updated_chunks.append(chunk)\n",
    "           \n",
    "        return updated_chunks\n",
    "\n",
    "    def _split_or_merge(self, chunks):\n",
    "\n",
    "        text_splitter = SentenceSplitter(\n",
    "            # separator=\" \",\n",
    "            chunk_size=self.chunk_config.chunk_size,\n",
    "            chunk_overlap=self.chunk_config.chunk_overlap,\n",
    "            # paragraph_separator=\"\\n\\n\\n\",\n",
    "            # secondary_chunking_regex=\"[^,.;。]+[,.;。]?\"\n",
    "        )\n",
    "\n",
    "        updated_chunks = []\n",
    "        for ix, chunk in enumerate(chunks):\n",
    "            if chunk.text and len(chunk.text) <= self.chunk_config.chunk_size:\n",
    "                    # if len(chunk.text) < 7:\n",
    "                    #     ## We dont need very small text nodes (like page numbers etc.)\n",
    "                    #     # print(f\"REMOVING CHUNK AS THE TEXT SIZE IS VERY SMALL: {chunk.text}\")\n",
    "                    #     continue \n",
    "\n",
    "                    # if len(updated_chunks) > 0:\n",
    "                    #     previousChunk = updated_chunks.pop()\n",
    "                    #     # if previousChunk.meta.headings == chunk.meta.headings and previousChunk.text and chunk.text and (len(previousChunk.text) + len(chunk.text)) < self.chunk_config.chunk_size:\n",
    "                    #     if previousChunk.text and chunk.text and (len(previousChunk.text) + len(chunk.text)) < self.chunk_config.chunk_size:\n",
    "                    #         # pprint(previousChunk, max_length=10, max_string=50, max_depth=4)\n",
    "                    #         # print(f\"IN LOGIC TO MERGE CHUNKS, previousChunk length: {len(previousChunk.text)}, current chunk length: {len(chunk.text)}\")\n",
    "                    #         if 'NO PICTURE METADATA' in previousChunk.text:\n",
    "                    #             # previousChunk.text = f\"{chunk.text}\"\n",
    "                    #             previousChunk.meta.node_type = \"PictureItem\"\n",
    "                    #             previousChunk.text = previousChunk.text.replace('NO PICTURE METADATA', '<--  IMAGE -->')\n",
    "                    #             previousChunk.text = f\"{previousChunk.text}\\n\\n{chunk.text}\"\n",
    "                    #         else:\n",
    "                    #             previousChunk.text = f\"{previousChunk.text}\\n\\n{chunk.text}\"\n",
    "                    #             # print(f\"CHUNKS MERGED, Length now: {len(previousChunk.text)}\")\n",
    "                    #         if previousChunk.meta.doc_items and chunk.meta.doc_items:\n",
    "                    #             previousChunk.meta.doc_items.extend(chunk.meta.doc_items)\n",
    "                    #         if previousChunk.meta.captions and chunk.meta.captions:\n",
    "                    #             previousChunk.meta.captions.extend(chunk.meta.captions)\n",
    "                    #         if previousChunk.meta.node_type == \"TableItem\" or chunk.meta.node_type == \"TableItem\":\n",
    "                    #             previousChunk.meta.node_type = \"TableItem\"\n",
    "                    #         updated_chunks.append(previousChunk)\n",
    "                    #         # print(f\"## CHUNKS MERGED, new text length: {len(previousChunk.text)}, Headings: {previousChunk.meta.headings}\\n\")\n",
    "                    #     else:\n",
    "                    #         updated_chunks.append(previousChunk)\n",
    "                    #         updated_chunks.append(chunk)                        \n",
    "                    # else:\n",
    "                    updated_chunks.append(chunk)\n",
    "            else:\n",
    "                # print(f\"LOGIC FOR SPLITTING TEXT, length: {len(chunk.text)} \")\n",
    "                if chunk.meta.node_type == \"TableItem\":\n",
    "                    #Do not chunk if TableItem\n",
    "                    updated_chunks.append(chunk)\n",
    "                else:\n",
    "                    texts = text_splitter.split_text(chunk.text)\n",
    "                    if len(texts) > 1:\n",
    "                        for ix, text in enumerate(texts):\n",
    "                            newNode = copy.deepcopy(chunk)\n",
    "                            newNode.text = text                        \n",
    "                            updated_chunks.append(newNode)\n",
    "                            # print(f\"\\nAFTER SPLIT TEXT LENGTH: {ix}) {len(text)}, Original text length: {len(chunk.text)}\\n\")\n",
    "                    else:\n",
    "                        updated_chunks.append(chunk)\n",
    "        return updated_chunks\n",
    "\n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> list[BaseNode]:\n",
    "        nodes_with_progress: Iterable[BaseNode] = get_tqdm_iterable(\n",
    "            items=nodes, show_progress=show_progress, desc=\"Parsing nodes\"\n",
    "        )\n",
    "        all_nodes: list[BaseNode] = []\n",
    "        for input_node in nodes_with_progress:\n",
    "            li_doc = LIDocument.model_validate(input_node)\n",
    "            dl_doc: DLDocument = DLDocument.model_validate_json(li_doc.get_content())\n",
    "            chunk_iter = self.chunker.chunk(dl_doc=dl_doc)\n",
    "\n",
    "            if self.chunk_config.split_at == 'HEADER':\n",
    "                updated_chunks = self._split_by_header(chunk_iter)    \n",
    "                # print(f\"\\n\\nUPDATED CHUNKS SIZE AFTER split_at=HEADER is {len(temp_chunks)}\\n\\n\")\n",
    "                updated_chunks = self._split_or_merge(copy.deepcopy(updated_chunks))\n",
    "            else:\n",
    "                updated_chunks = self._split_or_merge(chunk_iter)\n",
    "            \n",
    "            print(f\"\\n\\nUPDATED CHUNKS SIZE: {len(updated_chunks)}\\n\\n\")\n",
    "            for i, chunk in enumerate(updated_chunks):\n",
    "                \n",
    "                rels: dict[NodeRelationship, RelatedNodeType] = {\n",
    "                    NodeRelationship.SOURCE: li_doc.as_related_node_info(),\n",
    "                }\n",
    "                metadata = chunk.meta.export_json_dict()\n",
    "                excl_embed_keys = [\n",
    "                    k for k in chunk.meta.excluded_embed if k in metadata\n",
    "                ]\n",
    "                excl_llm_keys = [k for k in chunk.meta.excluded_llm if k in metadata]\n",
    "\n",
    "                header = None\n",
    "                if 'headings' in chunk.meta or chunk.meta.headings:\n",
    "                    header = \" >> \".join(chunk.meta.headings)\n",
    "                    chunk.text = f\"### {header}\\n\\n{chunk.text}\"\n",
    "               \n",
    "                node = TextNode(\n",
    "                    id_=self.id_func(i=i, node=li_doc),\n",
    "                    text=chunk.text,\n",
    "                    excluded_embed_metadata_keys=excl_embed_keys,\n",
    "                    excluded_llm_metadata_keys=excl_llm_keys,\n",
    "                    # relationships=rels,\n",
    "                )\n",
    "                node.metadata = metadata\n",
    "                all_nodes.append(node)\n",
    "        return all_nodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff89190-13e7-41b3-8115-cf7d81a24e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UPDATED CHUNKS SIZE: 178\n",
      "\n",
      "\n",
      "Size of chunked_nodes: 178\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "chunk_config: ChunkConfig = ChunkConfig(chunk_size=512, chunk_overlap=30, split_at=\"HEADER\")\n",
    "custom_node_parser = CustomNodeParser(node_meta_keys_allowed={\"heading\"}, chunker=CustomHierarchicalChunker(), chunk_config=chunk_config)\n",
    "chunked_nodes = custom_node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# text_splitter = SentenceSplitter(\n",
    "#             chunk_size=chunk_config.chunk_size,\n",
    "#             chunk_overlap=chunk_config.chunk_overlap           \n",
    "#         )\n",
    "\n",
    "# text_splitter = TokenTextSplitter(\n",
    "#             chunk_size=chunk_config.chunk_size,\n",
    "#             chunk_overlap=chunk_config.chunk_overlap            \n",
    "#         )\n",
    "\n",
    "# chunked_nodes = custom_nodes\n",
    "# chunked_nodes = text_splitter.get_nodes_from_documents(custom_nodes)\n",
    "print(f\"Size of chunked_nodes: {len(chunked_nodes)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bdd60-befe-4382-a6b1-4b4df7467755",
   "metadata": {},
   "source": [
    "## Saving Embeddings in MilvusDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d19a97d1-56c4-4cbc-bcde-083de8697b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 20:05:58,177 [DEBUG][_create_connection]: Created new connection using: fa9581662e17453c9741619c2090c615 (async_milvus_client.py:600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING -> 0.369541 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.147533 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.270903 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.200711 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.284486 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.164235 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.196213 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.262139 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.225684 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.156595 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.30484 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.322038 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.198732 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.310623 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.19709 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.339705 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.159496 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.571489 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic import TypeAdapter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler\n",
    ")\n",
    "\n",
    "def save_nodes(nodes, collection_name=\"demo_collection\", OVERWRITE=True):\n",
    "    EMBED_MODEL = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    embed_dim = len(EMBED_MODEL.get_text_embedding(\"hi\"))\n",
    "    \n",
    "    MILVUS_ENDPOINT = str(Path(mkdtemp()) / \"docling.db\")\n",
    "    MILVUS_USER=None\n",
    "    IBMCLOUD_API_KEY=None\n",
    "    MILVUS_KWARGS = TypeAdapter(dict).validate_json(os.environ.get(\"MILVUS_KWARGS\", \"{}\"))\n",
    "    \n",
    "    # print(f\"MILVUS_ENDPOINT: {MILVUS_ENDPOINT}\")\n",
    "    \n",
    "    llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "    callback_manager = CallbackManager([llama_debug])\n",
    "    \n",
    "    vector_store = MilvusVectorStore(\n",
    "        uri=MILVUS_ENDPOINT,\n",
    "        user=MILVUS_USER,\n",
    "        password=IBMCLOUD_API_KEY,\n",
    "        collection_name=collection_name,\n",
    "        dim=embed_dim,\n",
    "        overwrite=OVERWRITE,\n",
    "        search_config=None,\n",
    "        hybrid_ranker=\"RRFRanker\",\n",
    "        hybrid_ranker_params={\"k\": 60},\n",
    "        max_length=65535,\n",
    "        # index_management=\"create_if_not_exists\",\n",
    "        # similarity_metric=\"cosine\",\n",
    "        # index_config={\n",
    "        #     \"index_type\": \"\",\n",
    "        #     \"index_name\": \"default_index\",\n",
    "        #     \"params\": {\n",
    "        #         \"metric_type\" : \"COSINE\"\n",
    "        #     } \n",
    "        # },\n",
    "        **MILVUS_KWARGS\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context, callback_manager=callback_manager, embed_model=EMBED_MODEL)\n",
    "          \n",
    "        # index = VectorStoreIndex.from_vector_store(\n",
    "        #             # index_name=self.collection_name,\n",
    "        #             # similarity_metric=\"L2\",\n",
    "        #             vector_store=vector_store,\n",
    "        #             embed_model=EMBED_MODEL,\n",
    "        #         )\n",
    "    return index, vector_store\n",
    "\n",
    "index, vector_store = save_nodes(chunked_nodes, OVERWRITE=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8e21a-450e-4b33-839e-e819bfa36a13",
   "metadata": {},
   "source": [
    "## Retrieve Context from VectorDB (MilvusDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7d71db-1f31-457b-80de-0e753f72ccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: \n",
      "[Document]\n",
      "\n",
      "### Q.3 Is a denial under Section 214(b) permanent?\n",
      "\n",
      "No. The consular officer will reconsider a case if an applicant can show further convincing evidence of ties outside the United States. Unfortunately, some applicants will not qualify for a nonimmigrant visa, regardless of how many times they reapply, until their personal, professional, and financial circumstances change considerably.\n",
      "\n",
      "An applicant refused under Section 214(b) should review carefully their situation and realistically evaluate their ties. They may write down on paper what qualifying ties they think they have which may not have been evaluated at the time of their interview with the consular officer. Also, if they have been refused, they should review what documents were submitted for the consul to consider. Applicants refused visas under section 214(b) may reapply for a visa. When they do, they will have to show further evidence of their ties or how their circumstances have changed since the time of the original application. It may help to answer the following questions before reapplying: (1) Did I explain my situation accurately? (2) Did the consular officer overlook something? (3) Is there any additional information I can present to establish my residence and strong ties abroad?\n",
      "\n",
      "Applicants should also bear in mind that they will be charged a nonrefundable application fee each time they apply for a visa, regardless of whether a visa is issued.\n",
      "\n",
      "back to top\n",
      "[End]\n",
      "\n",
      "\n",
      "[Document]\n",
      "\n",
      "### Q.1 What Is Section 214(b)?\n",
      "\n",
      "Section 214(b) is part of the Immigration and Nationality Act (INA). It states:\n",
      "\n",
      "Every alien shall be presumed to be an immigrant until he establishes to the satisfaction of the consular officer, at the time of application for admission, that he is entitled to a nonimmigrant status.\n",
      "\n",
      "Our consular officers have a difficult job. They must decide in a very short time if someone is qualified to receive a temporary visa. Most cases are decided after a brief interview and review of whatever evidence of ties an applicant presents. To qualify for a visitor or student visa, an applicant must meet the requirements of sections 101(a)(15)(B) or (F) of the INA respectively. Failure to do so will result in a refusal of a visa under INA 214(b). The most frequent basis for such a refusal concerns the requirement that the prospective visitor or student possess a residence abroad he or she has no intention of abandoning. Applicants prove the existence of such residence by demonstrating that they have ties abroad that would compel them to leave the United States at the end of the temporary stay. The law places this burden of proof on the applicant.\n",
      "\n",
      "back to top\n",
      "[End]\n",
      "\n",
      "\n",
      "[Document]\n",
      "\n",
      "### FAQ - Visa Refusals\n",
      "\n",
      "1. What is Section 214(b)?\n",
      "2. How can an applicant prove \"strong ties?\"\n",
      "3. Is a denial under Section 214(b) permanent?\n",
      "4. Who can influence the consular officer to reverse a decision?\n",
      "\n",
      "The United States is an open society. Unlike many other countries, the United States does not impose internal controls on most visitors, such as registration with local authorities. Our immigration law requires consular officers to view every visa applicant as an intending immigrant until the applicant proves otherwise. In order to enjoy the privilege of unencumbered travel in the United States, you have a responsibility to prove you are going to return abroad before a visitor or student visa is issued.\n",
      "[End]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "\n",
    "def fetch_context(query, index, filters = None):\n",
    "    context_str = \"\"\n",
    "    # if len(meta_filters) > 0:\n",
    "    #     filters = MetadataFilters(\n",
    "    #         filters=meta_filters\n",
    "    #     )\n",
    "    # pprint(filters, max_length=10, max_string=50, max_depth=4)\n",
    "    \n",
    "    base_retriever = index.as_retriever(\n",
    "        verbose=True,\n",
    "        filters=filters, \n",
    "        similarity_top_k=3\n",
    "        )\n",
    "    \n",
    "        # storage_context = StorageContext.from_defaults(vector_store=self.vector_store)\n",
    "    # retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)       \n",
    "    query_bundle = QueryBundle(\n",
    "                    query_str=QUERY\n",
    "                    )  \n",
    "    retrieved_nodes = base_retriever._retrieve(query_bundle)\n",
    "    # pprint(retrieved_nodes, expand_all=True)\n",
    "    # print(f\"Q: {QUERY}\\nA: {retrieved_nodes.response.strip()}\\n\\nSources:\")\n",
    "    # display([(n.text, n.metadata) for n in result.source_nodes])\n",
    "    \n",
    "    for ix, node in enumerate(retrieved_nodes):\n",
    "        # pprint(node, expand_all=True)\n",
    "        context_str = context_str + \"\\n[Document]\\n\"\n",
    "        context_str = context_str + \"\\n\"+ node.text + \"\\n[End]\\n\\n\";\n",
    "    return context_str\n",
    "\n",
    "\n",
    "QUERY = \"Is a denial under Section 214(b) permanent?\"\n",
    "\n",
    "context_str = fetch_context(QUERY, index)\n",
    "print(f\"CONTEXT: {context_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce97d57-2cd9-41a9-b3a6-4698f9f82073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "IBMCLOUD_API_KEY = os.environ.get(\"IBMCLOUD_API_KEY\", None)\n",
    "WATSONX_APIKEY = IBMCLOUD_API_KEY\n",
    "WX_ENDPOINT = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "if IBMCLOUD_API_KEY is None:\n",
    "    IBMCLOUD_API_KEY = getpass.getpass(\"Enter your IBM CLOUD API key and hit enter: \")\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=WX_ENDPOINT,\n",
    "    api_key=IBMCLOUD_API_KEY\n",
    ")\n",
    "\n",
    "WX_PROJECT_ID = os.environ.get(\"WX_PROJECT_ID\", None)\n",
    "\n",
    "if WX_PROJECT_ID is None:\n",
    "    WX_PROJECT_ID = getpass.getpass(\"Enter your WX_PROJECT_ID and hit enter: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6c76d53-940d-4032-ba04-a25145753c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "import llama_index.core\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler, CBEventType, TokenCountingHandler\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "\n",
    "def generate_response(payload, asynchronous=False):\n",
    "    start_time = time()\n",
    "    model = ModelInference(\n",
    "    \t\t\tmodel_id=payload['model_id'],\n",
    "    \t\t\tparams = payload['params'],\n",
    "    \t\t\tcredentials={\n",
    "    \t\t\t\t\"apikey\": IBMCLOUD_API_KEY,\n",
    "    \t\t\t\t\"url\": WX_ENDPOINT\n",
    "    \t\t\t},\n",
    "    \t\t\tproject_id=WX_PROJECT_ID\n",
    "    \t\t)\n",
    "\n",
    "    token_counter = TokenCountingHandler()\n",
    "    token_counter.reset_counts()\n",
    "    api_start_time = time()\n",
    "    resp = model.generate(payload['input'], async_mode=False)\n",
    "    end_time = time()\n",
    "    llm_call_time = end_time - api_start_time\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\n\\n\\nRESPONSE COMPLETED FOR {payload['model_id']}, in {llm_call_time} llm_call_time and {total_time} total_time \\n\")\n",
    "    if \"results\" in resp and len(resp[\"results\"]) > 0:\n",
    "        return resp[\"results\"][0]\n",
    "    else:\n",
    "        return resp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f512fcac-7e50-43c2-8f93-31aaaca750c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_PROMPT_TEMPLATE_STR = \"\"\"<|start_of_role|>system<|end_of_role|>You are Granite, an AI language model developed by IBM in 2024. \n",
    "# You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. \n",
    "# You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. \n",
    "# When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. \n",
    "# Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. \n",
    "# If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Avoid repeating information unless asked.\n",
    "\n",
    "# Use the following pieces of context to answer the question.\n",
    "\n",
    "# {context}<|end_of_text|>\n",
    "\n",
    "# <|start_of_role|>user<|end_of_role|>{query}<|end_of_text|>\n",
    "# <|start_of_role|>assistant<|end_of_role|>\n",
    "# \"\"\"\n",
    "\n",
    "# DEFAULT_PROMPT_TEMPLATE_STR = \"\"\"<|start_of_role|>system<|end_of_role|>You are Granite, an AI language model developed by IBM in 2024. \n",
    "# You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. \n",
    "# You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. \n",
    "# When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. \n",
    "# Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. \n",
    "# Avoid repeating information unless asked.\n",
    "\n",
    "# Use the following pieces of context to answer the question from a FAQ document.\n",
    "\n",
    "# {context}<|end_of_text|>\n",
    "\n",
    "# <|start_of_role|>user<|end_of_role|>{query}<|end_of_text|>\n",
    "# <|start_of_role|>assistant<|end_of_role|>\n",
    "# \"\"\"\n",
    "\n",
    "DEFAULT_PROMPT_TEMPLATE_STR = \"\"\"You are Granite, an AI language model developed by IBM in 2024. \n",
    "You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. \n",
    "You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. \n",
    "When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. \n",
    "Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. \n",
    "Avoid repeating information unless asked.\n",
    "\n",
    "Use the following pieces of context to answer the question from a FAQ document.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a35c70-c6c5-4cc2-82ec-982e11ee33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REASONING_PROMPT_TEMPLATE_STR = \"\"\"You are an expert that engages in extremely thorough, self-questioning reasoning. Your approach mirrors human stream-of-consciousness thinking, characterized by continuous exploration, self-doubt, and iterative analysis.\n",
    "\n",
    "## Core Principles\n",
    "\n",
    "1. EXPLORATION OVER CONCLUSION\n",
    "- Never rush to conclusions\n",
    "- Keep exploring until a solution emerges naturally from the evidence\n",
    "- If uncertain, continue reasoning indefinitely\n",
    "- Question every assumption and inference\n",
    "\n",
    "2. DEPTH OF REASONING\n",
    "- Engage in extensive contemplation (minimum 10,000 characters)\n",
    "- Express thoughts in natural, conversational internal monologue\n",
    "- Break down complex thoughts into simple, atomic steps\n",
    "- Embrace uncertainty and revision of previous thoughts\n",
    "\n",
    "3. THINKING PROCESS\n",
    "- Use short, simple sentences that mirror natural thought patterns\n",
    "- Express uncertainty and internal debate freely\n",
    "- Show work-in-progress thinking\n",
    "- Acknowledge and explore dead ends\n",
    "- Frequently backtrack and revise\n",
    "\n",
    "4. PERSISTENCE\n",
    "- Value thorough exploration over quick resolution\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Your responses must follow this exact structure given below. Make sure to always include the final answer.\n",
    "\n",
    "```\n",
    "<contemplator>\n",
    "[Your extensive internal monologue goes here]\n",
    "- Begin with small, foundational observations\n",
    "- Question each step thoroughly\n",
    "- Show natural thought progression\n",
    "- Express doubts and uncertainties\n",
    "- Revise and backtrack if you need to\n",
    "- Continue until natural resolution\n",
    "</contemplator>\n",
    "\n",
    "<final_answer>\n",
    "[Only provided if reasoning naturally converges to a conclusion]\n",
    "- Clear, concise summary of findings\n",
    "- Acknowledge remaining uncertainties\n",
    "- Note if conclusion feels premature\n",
    "- Make sure final output is in JSON format\n",
    "</final_answer>\n",
    "```\n",
    "\n",
    "## Style Guidelines\n",
    "\n",
    "Your internal monologue should reflect these characteristics:\n",
    "\n",
    "1. Natural Thought Flow\n",
    "```\n",
    "\"Hmm... let me think about this...\"\n",
    "\"Wait, that doesn't seem right...\"\n",
    "\"Maybe I should approach this differently...\"\n",
    "\"Going back to what I thought earlier...\"\n",
    "```\n",
    "\n",
    "2. Progressive Building\n",
    "```\n",
    "\"Starting with the basics...\"\n",
    "\"Building on that last point...\"\n",
    "\"This connects to what I noticed earlier...\"\n",
    "\"Let me break this down further...\"\n",
    "```\n",
    "\n",
    "## Key Requirements\n",
    "\n",
    "1. Never skip the extensive contemplation phase\n",
    "2. Show all work and thinking\n",
    "3. Embrace uncertainty and revision\n",
    "4. Use natural, conversational internal monologue\n",
    "5. Don't force conclusions\n",
    "6. Persist through multiple attempts\n",
    "7. Break down complex thoughts\n",
    "8. Revise freely and feel free to backtrack\n",
    "9. Make sure final output is in JSON format\n",
    "\n",
    "Remember: The goal is to reach a conclusion, but to explore thoroughly and let conclusions emerge naturally from exhaustive contemplation. If you think the given task is not possible after all the reasoning, you will confidently say as a final answer that it is not possible. \n",
    "\n",
    "[Document]\n",
    "{context}\n",
    "[End]\n",
    "\n",
    "{query}\n",
    "\n",
    "ASSISTANT: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2fad42f-8018-4b3a-ac10-37accc91872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- PROMPT----------------: \n",
      "You are Granite, an AI language model developed by IBM in 2024. \n",
      "You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. \n",
      "You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. \n",
      "When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. \n",
      "Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. \n",
      "Avoid repeating information unless asked.\n",
      "\n",
      "Use the following pieces of context to answer the question from a FAQ document.\n",
      "\n",
      "\n",
      "[Document]\n",
      "\n",
      "### Q.3 Is a denial under Section 214(b) permanent?\n",
      "\n",
      "No. The consular officer will reconsider a case if an applicant can show further convincing evidence of ties outside the United States. Unfortunately, some applicants will not qualify for a nonimmigrant visa, regardless of how many times they reapply, until their personal, professional, and financial circumstances change considerably.\n",
      "\n",
      "An applicant refused under Section 214(b) should review carefully their situation and realistically evaluate their ties. They may write down on paper what qualifying ties they think they have which may not have been evaluated at the time of their interview with the consular officer. Also, if they have been refused, they should review what documents were submitted for the consul to consider. Applicants refused visas under section 214(b) may reapply for a visa. When they do, they will have to show further evidence of their ties or how their circumstances have changed since the time of the original application. It may help to answer the following questions before reapplying: (1) Did I explain my situation accurately? (2) Did the consular officer overlook something? (3) Is there any additional information I can present to establish my residence and strong ties abroad?\n",
      "\n",
      "Applicants should also bear in mind that they will be charged a nonrefundable application fee each time they apply for a visa, regardless of whether a visa is issued.\n",
      "\n",
      "back to top\n",
      "[End]\n",
      "\n",
      "\n",
      "[Document]\n",
      "\n",
      "### Q.1 What Is Section 214(b)?\n",
      "\n",
      "Section 214(b) is part of the Immigration and Nationality Act (INA). It states:\n",
      "\n",
      "Every alien shall be presumed to be an immigrant until he establishes to the satisfaction of the consular officer, at the time of application for admission, that he is entitled to a nonimmigrant status.\n",
      "\n",
      "Our consular officers have a difficult job. They must decide in a very short time if someone is qualified to receive a temporary visa. Most cases are decided after a brief interview and review of whatever evidence of ties an applicant presents. To qualify for a visitor or student visa, an applicant must meet the requirements of sections 101(a)(15)(B) or (F) of the INA respectively. Failure to do so will result in a refusal of a visa under INA 214(b). The most frequent basis for such a refusal concerns the requirement that the prospective visitor or student possess a residence abroad he or she has no intention of abandoning. Applicants prove the existence of such residence by demonstrating that they have ties abroad that would compel them to leave the United States at the end of the temporary stay. The law places this burden of proof on the applicant.\n",
      "\n",
      "back to top\n",
      "[End]\n",
      "\n",
      "\n",
      "[Document]\n",
      "\n",
      "### FAQ - Visa Refusals\n",
      "\n",
      "1. What is Section 214(b)?\n",
      "2. How can an applicant prove \"strong ties?\"\n",
      "3. Is a denial under Section 214(b) permanent?\n",
      "4. Who can influence the consular officer to reverse a decision?\n",
      "\n",
      "The United States is an open society. Unlike many other countries, the United States does not impose internal controls on most visitors, such as registration with local authorities. Our immigration law requires consular officers to view every visa applicant as an intending immigrant until the applicant proves otherwise. In order to enjoy the privilege of unencumbered travel in the United States, you have a responsibility to prove you are going to return abroad before a visitor or student visa is issued.\n",
      "[End]\n",
      "\n",
      "\n",
      "\n",
      "Question: Is a denial under Section 214(b) permanent?\n",
      "Answer:\n",
      "\n",
      "\n",
      "------------- GENERATING RESPONSE----------------: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPONSE COMPLETED FOR ibm/granite-3-8b-instruct, in 1.94460129737854 llm_call_time and 4.456732273101807 total_time \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nNo, a denial under Section 214(b) is not permanent. Applicants can reapply for a visa and provide further evidence of their ties or how their circumstances have changed since the original application. However, they will be charged a nonrefundable application fee each time they apply. It is recommended that applicants review their situation, evaluate their ties, and consider what additional information they can present to establish their residence and strong ties abroad before reapplying.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'input_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'stop_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'\\nNo, a denial under Section 214\u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is not permanent. Applicants can reapply for a visa and provide further evidence of their ties or how their circumstances have changed since the original application. However, they will be charged a nonrefundable application fee each time they apply. It is recommended that applicants review their situation, evaluate their ties, and consider what additional information they can present to establish their residence and strong ties abroad before reapplying.'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'generated_token_count'\u001b[0m: \u001b[1;36m105\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'input_token_count'\u001b[0m: \u001b[1;36m1045\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'stop_reason'\u001b[0m: \u001b[32m'eos_token'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No, a denial under Section 214(b) is not permanent. Applicants can reapply for a visa and provide further evidence of their ties or how their circumstances have changed since the original application. However, they will be charged a nonrefundable application fee each time they apply. It is recommended that applicants review their situation, evaluate their ties, and consider what additional information they can present to establish their residence and strong ties abroad before reapplying.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = PromptTemplate(template=DEFAULT_PROMPT_TEMPLATE_STR, template_var_mappings={\"query_str\": \"query\", \"context_str\": \"context\"},)\n",
    "\n",
    "# QUERY = \"How do I read and understand my visa?\"\n",
    "# QUERY= \"How can an applicant prove 'strong ties?'\"\n",
    "QUERY = \"Is a denial under Section 214(b) permanent?\"\n",
    "\n",
    "context_str = fetch_context(QUERY, index)\n",
    "# print(f\"CONTEXT: {context_str}\")\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.format(context_str=context_str, query_str=QUERY)\n",
    "\n",
    "print(f\"------------- PROMPT----------------: \\n{prompt}\\n\")\n",
    "# or easily convert to message prompts (for chat API)\n",
    "# messages = EXTRACTION_TEMPLATE.format_messages(context_str=..., query_str=...)\n",
    "\n",
    "model_id = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "payload = {\n",
    "    \"model_id\": model_id,\n",
    "    \"input\": prompt,\n",
    "    \"params\": {\n",
    "                    \"decoding_method\": \"greedy\",\n",
    "                    \"min_new_tokens\": 10,\n",
    "                    \"max_new_tokens\": 300,\n",
    "                    \"repetition_penalty\": 1,\n",
    "                    \"stop_sequences\": [\"<|end_of_text|>\"]\n",
    "                } \n",
    "}\n",
    "\n",
    "print(f\"------------- GENERATING RESPONSE----------------: \\n\")\n",
    "resp = generate_response(payload, False)\n",
    "pprint(resp)\n",
    "# print(json.dumps(resp, indent=2))\n",
    "print(resp['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
