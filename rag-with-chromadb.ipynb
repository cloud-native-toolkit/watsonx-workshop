{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Retrieval Augmented Generation using watsonx.ai and Vector Database\n\nIn this notebook, we'll demonstrate how to utilize a Vector Database to retrieve relevant passages based on a user query. We'll then append these passages as context to the prompt that will be passed to the LLM in watsonx.ai for generation."}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction\n\nRetrieval Augmented Generation (RAG) is a powerful technique that combines the strengths of pre-trained large language models (LLM) and information retrieval systems to generate responses based on a given context. In this notebook, we will be using a Vector Database and watsonx.ai foundation models to implement a RAG use-case.\n\nA vector database (or store), when applied to text data, is a specialized database that efficiently stores embeddings, representing pieces of text, for efficient  queries. It enables quick similarity searches, allowing you to pinpoint texts that are _'similar'_ based on their vectorized representations. For our purposes, we will use Chroma, an open-source embedding database.\n\nInstead of using Watson Discovery to pass back the relevant passages, we are using a vector database called Chroma. Chroma is mainly used to parse through the PDFs, store the content, and then query from that collection. The code in the notebook below demonstrates the implementation of this approach."}, {"metadata": {}, "cell_type": "markdown", "source": "### Pre-requisites\n\nThis lab should take about 45 minutes.\n\nBefore we begin lets start off by ensuring we have completed some pre-requisites; ensure you gave the following\n\n- IBM Cloud API key \n- Project ID associated with your watsonx instance\n\nYou can use the following support links if you need any help with the pre-requisites above\n\n- [Creating IBM Cloud API Key](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key)\n- [Finding watsonx Project ID](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=library-project-id)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Setting up"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Importing Required Libraries\n\nBefore we get started looking at some code, we will need to install some dependencies for our notebook; the following notebook cell will do just that."}, {"metadata": {}, "cell_type": "code", "source": "# Download dependencies\n\n# import sys\n# !{sys.executable} -m pip install -q langchain\n# !{sys.executable} -m pip install -q chromadb\n# !{sys.executable} -m pip install -q pypdf\n\n# !{sys.executable} -m pip install -q ibm_cloud_sdk_core\n# !{sys.executable} -m pip install -q ibm_watson_machine_learning\n\n!pip install -U langchain==0.0.312\n!pip install -U chromadb==0.4.2\n!pip install -U pypdf==3.12.2\n\n!pip install -U ibm_cloud_sdk_core\n!pip install -U ibm_watson_machine_learning==1.0.327\n!pip install -U sentence-transformers==2.2.2\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: langchain==0.0.312 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.0.312)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (3.9.0)\nRequirement already satisfied: anyio<4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (3.7.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.33)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.43 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (0.0.87)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.10.14)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.8.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.2.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.312) (2.4)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.312) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.312) (2.0.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (23.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (0.4.3)\nRequirement already satisfied: chromadb==0.4.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.5.3)\nRequirement already satisfied: requests>=2.28 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (2.31.0)\nRequirement already satisfied: pydantic<2.0,>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.10.14)\nRequirement already satisfied: chroma-hnswlib==0.7.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (0.7.1)\nRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (0.99.1)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (0.27.0.post1)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.23.5)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (3.4.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (4.9.0)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (3.4.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.17.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (0.14.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (4.65.0)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (6.1.1)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.2) (0.27.0)\nRequirement already satisfied: coloredlogs in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (2.0)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (23.0)\nRequirement already satisfied: protobuf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (4.21.12)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.2) (1.16.0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.2) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.2) (2.2.1)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.2) (2023.11.17)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (1.26.18)\nRequirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.4.2) (0.17.3)\nRequirement already satisfied: click>=7.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.2) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.2) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (1.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (6.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (12.0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.2) (3.9.0)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.2) (2024.2.0)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2) (3.7.1)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.2) (10.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.2) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2) (1.2.0)\nRequirement already satisfied: pypdf==3.12.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (3.12.2)\nRequirement already satisfied: ibm_cloud_sdk_core in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (3.19.1)\nRequirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_cloud_sdk_core) (2.31.0)\nCollecting urllib3<3.0.0,>=2.1.0 (from ibm_cloud_sdk_core)\n  Using cached urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_cloud_sdk_core) (2.8.2)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_cloud_sdk_core) (2.8.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->ibm_cloud_sdk_core) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->ibm_cloud_sdk_core) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->ibm_cloud_sdk_core) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->ibm_cloud_sdk_core) (2023.11.17)\nUsing cached urllib3-2.2.0-py3-none-any.whl (120 kB)\nInstalling collected packages: urllib3\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbotocore 1.27.59 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.0 which is incompatible.\nibm-cos-sdk-core 2.12.0 requires urllib3<1.27,>=1.26.9, but you have urllib3 2.2.0 which is incompatible.\nibm-watson-openscale 3.0.34 requires ibm-cloud-sdk-core==3.16.5, but you have ibm-cloud-sdk-core 3.19.1 which is incompatible.\nibm-watsonx-ai 0.1.4 requires ibm-watson-machine-learning>=1.0.335, but you have ibm-watson-machine-learning 1.0.327 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed urllib3-2.2.0\nRequirement already satisfied: ibm_watson_machine_learning==1.0.327 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (1.0.327)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (2.31.0)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (2.2.0)\nRequirement already satisfied: pandas<1.6.0,>=0.24.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (1.5.3)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (2023.11.17)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (0.3.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (0.8.10)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (6.0.0)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm_watson_machine_learning==1.0.327) (2.12.0)\nRequirement already satisfied: ibm-cos-sdk-core==2.12.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.327) (2.12.0)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.12.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.327) (2.12.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.327) (0.10.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ibm-cos-sdk-core==2.12.0->ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.327) (2.8.2)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting urllib3 (from ibm_watson_machine_learning==1.0.327)\n  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm_watson_machine_learning==1.0.327) (2022.7)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm_watson_machine_learning==1.0.327) (1.23.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->ibm_watson_machine_learning==1.0.327) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->ibm_watson_machine_learning==1.0.327) (3.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from importlib-metadata->ibm_watson_machine_learning==1.0.327) (3.11.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from lomond->ibm_watson_machine_learning==1.0.327) (1.16.0)\nUsing cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\nInstalling collected packages: urllib3\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.2.0\n    Uninstalling urllib3-2.2.0:\n      Successfully uninstalled urllib3-2.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nibm-cloud-sdk-core 3.19.1 requires urllib3<3.0.0,>=2.1.0, but you have urllib3 1.26.18 which is incompatible.\nibm-watson-openscale 3.0.34 requires ibm-cloud-sdk-core==3.16.5, but you have ibm-cloud-sdk-core 3.19.1 which is incompatible.\nibm-watsonx-ai 0.1.4 requires ibm-watson-machine-learning>=1.0.335, but you have ibm-watson-machine-learning 1.0.327 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed urllib3-1.26.18\nRequirement already satisfied: sentence-transformers==2.2.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.34.0)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.1)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.2)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.1.1)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.10.1)\nRequirement already satisfied: nltk in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.8.1)\nRequirement already satisfied: sentencepiece in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.9.0)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.0)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (10.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "code", "source": "# Import necessary modules and packages\n\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_machine_learning.foundation_models import Model\n\nimport langchain.embeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders.pdf import PyPDFLoader\n\nfrom sentence_transformers import SentenceTransformer\nfrom typing import Optional, Iterable, List\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Embedding & Vector Database"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creating Embeddings Class\n\nTo start off we will create a custom class, **MiniLML6V2EmbeddingFunctionLangchain**, and define some functions which are designed to generate embeddings using the `MiniLM-L6-v2` model from the `sentence_transformers` library. This class will serve as our embedding function where text we want to store in vector format will be processed before being stored within a vector database. As a quick reminder, embeddings are used to create a vector representation of the text data and capture the semantic meaning."}, {"metadata": {}, "cell_type": "code", "source": "class MiniLML6V2EmbeddingFunctionLangchain(langchain.embeddings.openai.Embeddings):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def embed_documents(self, texts):\n        return MiniLML6V2EmbeddingFunctionLangchain.MODEL.encode(texts).tolist()\n    \n    def embed_query():\n        super().embed_query()\n \nprint('done')\n", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creating A VectorDB Class\n\nWe will also create a custom class, **ChromaWithUpsert**, which is an abstraction using `Chroma` class from the `Chroma` vectorstore class in the langchain module. Using this class we introduce the ability to _upsert_ texts within the vector database _(either adding or updating)_. The _upsert_texts_ method from our class takes in the text content, their metadata _(i.e. source document)_, and their ids _(if provided)_, and generates the embeddings using the class defined earlier before adding the newly created vector in to the `Chroma` vector database."}, {"metadata": {}, "cell_type": "code", "source": "class ChromaWithUpsert(Chroma):\n    def upsert_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        \n        if ids is None:\n            import uuid\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts = list(texts))\n\n        self._collection.upsert(\n            metadatas=metadatas, embeddings=embeddings, documents=texts, ids=ids\n        )\n        return ids\n    \n    def query(self, query_texts:str, n_results:int=5, include: Optional[List[str]]=None):\n        return self._collection.query(\n            query_texts=query_texts,\n            n_results=n_results,\n            include=include\n        )\n\nprint('done')\n", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Loading and Splitting PDF Text Content\n\nIn the following cell we are loading PDF documents using the **PyPDFLoader** class and storing it in the data variable. Our PDF is being loaded from a URL and will be used to represent our existing knowledge base.\n\nThe loaded data is then split into smaller chunks using the **RecursiveCharacterTextSplitter** class, which allows us to split long text on predefined characters that are considered potential division points . The size of the chunks and the overlap between them is defined by `CHUNK_SIZE` and `CHUNK_OVERLAP` variables."}, {"metadata": {}, "cell_type": "code", "source": "loader = PyPDFLoader(\"https://www.captiveaire.com/manuals/exhaustfans/exhaust-oim.pdf\")\ndata = loader.load()\n\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 10\n\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=CHUNK_SIZE,\n            chunk_overlap=CHUNK_OVERLAP\n        )\ntexts = text_splitter.split_documents(data)\nprint('done')\n", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving texts to a VectorDB\n\n\nOnce our loaded text is split we can now can create an instance of our vector database using the `ChromaWithUpsert` class with our custom embedding function and a collection name. Once defined, using the `upsert_texts` method, we add the split texts and their metadata to the vector database. "}, {"metadata": {}, "cell_type": "code", "source": "vector_store = ChromaWithUpsert(\n    collection_name=f\"store_minilm6v2\",\n    embedding_function=MiniLML6V2EmbeddingFunctionLangchain(),\n)\n\nvector_store.upsert_texts(\n        texts=[doc.page_content for doc in texts],\n        metadatas=[doc.metadata for doc in texts]\n)\nprint('done')\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Set up the Language Learning Model (LLM)\n\nIn this cell, we are setting up the parameters for the Language Learning Model (LLM). This includes our IBM Cloud API Key and watsonx Project ID in order to make use of `watsonx.ai` foundation models. Default tuning parameters (gen) are provided, but can be adjusted as needed; aAfter setting up these parameters, we will use them to initialize our LLM (watsonx.ai) in the next cell.\n\nIf you want to learn more about watsonx.ai foundation models tuning paremeter, you can visit the watsonx.ai foundation [documentation link here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-parameters.html?context=wx&audience=wdp)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Your IBM Cloud API key\napi_key = ''\n\n# Project ID of your watsonx project\nwatsonx_project_id = ''\n\n# LLM that we want to use with watsonx.ai\nmodel_id= \"google/flan-ul2\"\n\nendpoint= \"https://us-south.ml.cloud.ibm.com\"\n\naccess_token = ''\n\ntry:\n  access_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n  ).get_token()\nexcept:\n  print('Issue obtaining access token. Check variables?') \n\ncredentials = { \n    \"url\"    : endpoint, \n    \"token\" : access_token\n}\n\n# watsonx.ai tuning parameters\ngen_params = {\n    \"DECODING_METHOD\" : \"greedy\",\n    \"MAX_NEW_TOKENS\" : 300,\n    \"MIN_NEW_TOKENS\" : 1,\n    \"TEMPERATURE\" : 0.7,\n    \"TOP_K\" : 50,\n    \"TOP_P\" : 0.15,\n    \"REPETITION_PENALTY\" : 2.0\n}\n\nmodel = Model( model_id, credentials, gen_params, watsonx_project_id )\nprint('done')\n", "execution_count": 7, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'os' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlwdvc5Z0sCiZLv7r8GjxZ7aCY0GP5IO6PqQDRqNf5JKv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Project ID of your watsonx project\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m watsonx_project_id \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROJECT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# LLM that we want to use with watsonx.ai\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model_id\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-ul2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n", "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Combining watsonx.ai LLM and VectorDB"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Constrcut Query & Identify the relevant texts in the documents\n\nIn this cell, we are constructing the query prompt for the Language Learning Model (LLM). The query is the question that we want to ask our foundation model. This question will be used to retrieve the relevant texts from the documents in our vector database.\n\nWe specify the number of text passages we want returned from our vector database using the `search_k` variable _(in this case, we use 3)_. If you find that you are not getting very good answers, you can increase the `search_k` variable, in order to increase the amount of context (number of matching passages) provided to \n\nWe will store the best relevant text passage along with its metadata and distances, which identify the source and page number and join them all into our `context` variable."}, {"metadata": {}, "cell_type": "code", "source": "question = 'For power roof ventilators should dampers be installed when an exhauster is used?'\n\nsearch_k = 5\ndocs = []\ndocs = vector_store.query(\n            query_texts=[question],\n            n_results=search_k,\n            include=[\"documents\",\"metadatas\", \"distances\"]\n        )\n\ncontext = \" \".join(docs[\"documents\"][0])\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Construct the Prompt & Query watsonx.ai\n\nNow, we combine the query and the context we received from the vector database into a prompt. We created a custom function to take in both the query and context.\n\nWe will then query our foundation model from watsonx.ai that we created earlier; given that we _upserted_ the documents with the metadata of the source and documents, we can identify which document and where in that document that we are using text context from in order to answer the question."}, {"metadata": {}, "cell_type": "code", "source": "#######################################################################################\nprompt_template = \"\"\"\nAnswer the following question using the context provided. \nIf there is no good answer, say \"I don't know\".\n\nContext: %s\n\nQuestion: %s\n\"\"\"\n\n#######################################################################################\ndef augment( template_in, context_in, query_in ):\n    return template_in % ( context_in,  query_in )\n\n#######################################################################################\nimport json\n\ndef generate( model_in, augmented_prompt_in ):\n    \n    generated_response = model_in.generate( augmented_prompt_in )\n \n    if ( \"results\" in generated_response ) \\\n       and ( len( generated_response[\"results\"] ) > 0 ) \\\n       and ( \"generated_text\" in generated_response[\"results\"][0] ):\n        return generated_response[\"results\"][0][\"generated_text\"]\n    else:\n        print( \"The model failed to generate an answer\" )\n        print( \"\\nDebug info:\\n\" + json.dumps( generated_response, indent=3 ) )\n        return \"\"\n\n########################################################################################\nimport re\n\naugmented_prompt = augment( prompt_template, context, question)\noutput = generate( model, augmented_prompt )\nif not re.match( r\"\\S+\", output ):\n    print( \"The model failed to generate an answer\")\nprint( \"\\nAnswer:\\t\" + output )\n\nsource_file = docs['metadatas'][0][0]['source']\npage = docs['metadatas'][0][0]['page']\n\nprint('\\nSource\\t', source_file)\nprint('Page\\t',page)\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations you just completed a RAG implementaion using VectorDB. Feel free to re-run the prompt by asking other questions or change the PDF used to provide watsonx.ai with a different context. "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}